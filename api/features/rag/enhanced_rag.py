from typing import List, Dict, Any, Optional
import os
from pathlib import Path

# LangChain imports for advanced retrieval
from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.schema import Document

# Local imports
from api.features.store.vector_store import QdrantVectorStore
from api.domain.chatmodel import ChatOpenAI as LocalChatOpenAI
from api.domain.prompts import SystemRolePrompt, UserRolePrompt

# Enhanced RAG Constants
DEFAULT_SYSTEM_PROMPT = "You are a helpful AI assistant that answers questions based on the provided context."
DEFAULT_COLLECTION_NAME = "documents"
DEFAULT_MODEL_NAME = "gpt-4o-mini"
DEFAULT_K = 5
DEFAULT_CHUNK_SIZE = 1000
DEFAULT_CHUNK_OVERLAP = 200

# Response Templates
NO_RESULTS_RESPONSE = "I don't have any relevant information to answer your question."
ERROR_RESPONSE = "I encountered an error while searching for relevant information. Please try again."

class EnhancedRAGQueryEngine:
    """
    Enhanced RAG Query Engine with advanced chunking and retrieval methods
    Based on successful RAGAS evaluation results (95.8% faithfulness, 94.4% context precision)
    """
    
    def __init__(self, 
                 collection_name: str = DEFAULT_COLLECTION_NAME,
                 model_name: str = DEFAULT_MODEL_NAME,
                 k: int = DEFAULT_K,
                 retriever_type: str = "ensemble",
                 chunk_size: int = DEFAULT_CHUNK_SIZE,
                 chunk_overlap: int = DEFAULT_CHUNK_OVERLAP):
        """
        Initialize Enhanced RAG Query Engine
        
        Args:
            collection_name: Name of the vector store collection
            model_name: OpenAI model name
            k: Number of documents to retrieve
            retriever_type: Type of retriever ("ensemble", "bm25", "compression", "multi_query", "semantic")
            chunk_size: Size of document chunks
            chunk_overlap: Overlap between chunks
        """
        self.collection_name = collection_name
        self.model_name = model_name
        self.k = k
        self.retriever_type = retriever_type
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # Initialize components
        self.chat_model = LocalChatOpenAI(model_name=model_name)
        self.openai_llm = ChatOpenAI(model=model_name)
        self.embeddings = OpenAIEmbeddings()
        
        # Document storage
        self.docs = []
        self.retriever = None
        self.vectorstore = None
        
        print(f"üöÄ Enhanced RAG initialized with {retriever_type} retriever")
    
    def load_documents_from_directory(self, data_path: str, max_docs: Optional[int] = None, 
                                    max_file_size_mb: int = 30) -> bool:
        """
        Load and chunk documents from directory using advanced chunking strategies
        
        Args:
            data_path: Path to directory containing PDF files
            max_docs: Maximum number of documents to load
            max_file_size_mb: Maximum file size in MB
        
        Returns:
            bool: Success status
        """
        try:
            if not os.path.exists(data_path):
                print(f"‚ùå Data path does not exist: {data_path}")
                return False
            
            # Get all PDF files and filter by size
            pdf_files = []
            data_path_obj = Path(data_path)
            
            for pdf_file in data_path_obj.glob("*.pdf"):
                file_size_mb = pdf_file.stat().st_size / (1024 * 1024)
                if file_size_mb <= max_file_size_mb:
                    pdf_files.append(str(pdf_file))
                else:
                    print(f"‚ö†Ô∏è  Skipping {pdf_file.name} ({file_size_mb:.1f}MB) - exceeds {max_file_size_mb}MB limit")
            
            if not pdf_files:
                print("‚ùå No suitable PDF files found within size limit")
                return False
            
            print(f"üìÅ Found {len(pdf_files)} PDF files within size limit")
            
            # Load documents one by one
            all_docs = []
            for pdf_file in pdf_files:
                try:
                    loader = PyMuPDFLoader(pdf_file)
                    docs = loader.load()
                    all_docs.extend(docs)
                    print(f"‚úÖ Loaded {len(docs)} pages from {Path(pdf_file).name}")
                except Exception as e:
                    print(f"‚ö†Ô∏è  Error loading {Path(pdf_file).name}: {e}")
                    continue
            
            if max_docs and len(all_docs) > max_docs:
                all_docs = all_docs[:max_docs]
                print(f"üìÑ Limited to first {max_docs} documents")
            
            # Apply advanced chunking strategy
            print(f"üîÑ Applying advanced chunking strategy...")
            chunked_docs = self._apply_advanced_chunking(all_docs)
            
            self.docs = chunked_docs
            print(f"‚úÖ Total chunks created: {len(self.docs)}")
            
            # Create retriever
            self._create_retriever()
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error loading documents: {e}")
            return False
    
    def _apply_advanced_chunking(self, docs: List[Document]) -> List[Document]:
        """
        Apply advanced chunking strategies with content filtering
        Based on successful RAGAS evaluation methods
        """
        try:
            # Use recursive character splitter (best results from RAGAS eval)
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                separators=["\n\n", "\n", ". ", " ", ""]
            )
            
            # Split documents
            chunked_docs = text_splitter.split_documents(docs)
            
            # Filter out chunks that are likely headers/footers/TOC
            filtered_chunks = []
            for chunk in chunked_docs:
                content = chunk.page_content.strip()
                if len(content) > 50 and not self._is_likely_header_footer(content):
                    filtered_chunks.append(chunk)
            
            print(f"üìä Chunking results: {len(chunked_docs)} total ‚Üí {len(filtered_chunks)} filtered chunks")
            return filtered_chunks
            
        except Exception as e:
            print(f"‚ùå Error in chunking strategy: {e}")
            return docs
    
    def _is_likely_header_footer(self, text: str) -> bool:
        """
        Identify and filter out likely headers, footers, and table of contents
        Based on patterns that caused issues in RAGAS evaluation
        """
        text_lower = text.lower()
        
        # Common header/footer patterns
        header_footer_patterns = [
            "copyright", "all rights reserved", "page", "table of contents",
            "level 2 training guide", "crossfit, inc", "v5.1-", "workbook |"
        ]
        
        # TOC patterns
        toc_patterns = [
            "...", "summary sheet:", "course overview", "preparation for",
            "learning objectives", "schedule", "day 1", "day 2"
        ]
        
        # Check if text is mostly dots (TOC formatting)
        dot_ratio = text.count('.') / len(text) if len(text) > 0 else 0
        
        # Check patterns
        for pattern in header_footer_patterns + toc_patterns:
            if pattern in text_lower:
                return True
        
        # Check if mostly dots (TOC)
        if dot_ratio > 0.3:
            return True
        
        # Check if very short and contains numbers (page numbers)
        if len(text) < 100 and any(char.isdigit() for char in text):
            return True
        
        return False
    
    def _create_retriever(self):
        """
        Create advanced retriever based on specified type
        """
        if not self.docs:
            print("‚ùå No documents loaded for retriever creation")
            return
        
        try:
            print(f"üîÑ Creating {self.retriever_type} retriever...")
            
            # Create vector store
            self.vectorstore = FAISS.from_documents(self.docs, self.embeddings)
            
            if self.retriever_type == "bm25":
                # BM25 (keyword-based) retriever
                self.retriever = BM25Retriever.from_documents(self.docs)
                self.retriever.k = self.k
                
            elif self.retriever_type == "ensemble":
                # Ensemble of semantic + keyword retrieval (best results from RAGAS)
                semantic_retriever = self.vectorstore.as_retriever(search_kwargs={"k": self.k})
                bm25_retriever = BM25Retriever.from_documents(self.docs)
                bm25_retriever.k = self.k
                
                self.retriever = EnsembleRetriever(
                    retrievers=[semantic_retriever, bm25_retriever],
                    weights=[0.6, 0.4]  # Favor semantic over keyword
                )
                
            elif self.retriever_type == "compression":
                # Compression retriever to filter irrelevant context
                base_retriever = self.vectorstore.as_retriever(search_kwargs={"k": self.k*2})
                compressor = LLMChainExtractor.from_llm(self.openai_llm)
                self.retriever = ContextualCompressionRetriever(
                    base_compressor=compressor,
                    base_retriever=base_retriever
                )
                
            elif self.retriever_type == "multi_query":
                # Multi-query retriever for better question understanding
                base_retriever = self.vectorstore.as_retriever(search_kwargs={"k": self.k})
                self.retriever = MultiQueryRetriever.from_llm(
                    retriever=base_retriever,
                    llm=self.openai_llm
                )
                
            elif self.retriever_type == "parent":
                # Parent document retriever for better context
                parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)
                child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)
                
                store = InMemoryStore()
                self.retriever = ParentDocumentRetriever(
                    vectorstore=self.vectorstore,
                    docstore=store,
                    child_splitter=child_splitter,
                    parent_splitter=parent_splitter,
                )
                self.retriever.add_documents(self.docs)
                
            else:
                # Default to semantic retriever
                self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": self.k})
            
            print(f"‚úÖ {self.retriever_type.title()} retriever created successfully")
            
        except Exception as e:
            print(f"‚ùå Error creating {self.retriever_type} retriever: {e}")
            print("üîÑ Falling back to basic semantic retriever...")
            
            # Try to create a basic vectorstore without FAISS
            try:
                from langchain_community.vectorstores import Chroma
                # Use Chroma as fallback (doesn't require FAISS)
                self.vectorstore = Chroma.from_documents(self.docs, self.embeddings)
                self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": self.k})
                print("‚úÖ Fallback semantic retriever created successfully")
            except Exception as fallback_error:
                print(f"‚ùå Fallback retriever also failed: {fallback_error}")
                self.retriever = None
    
    def query(self, query: str, system_prompt: Optional[str] = None) -> Dict[str, Any]:
        """
        Query the enhanced RAG system
        
        Args:
            query: User question
            system_prompt: Optional system prompt
        
        Returns:
            Dict with answer and sources
        """
        if not self.retriever:
            return {
                "answer": "RAG system not initialized. Please load documents first.",
                "sources": []
            }
        
        try:
            # Retrieve relevant documents using advanced retriever
            retrieved_docs = self.retriever.get_relevant_documents(query)
            
            if not retrieved_docs:
                return {
                    "answer": NO_RESULTS_RESPONSE,
                    "sources": []
                }
            
            # Format context
            context = self._format_context(retrieved_docs)
            
            # Create messages
            messages = [
                SystemRolePrompt(system_prompt or DEFAULT_SYSTEM_PROMPT).create_message(),
                UserRolePrompt(
                    f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:"
                ).create_message()
            ]
            
            # Get response
            response = self.chat_model.run(messages)
            
            # Extract sources
            sources = [{
                "text": doc.page_content,
                "source": doc.metadata.get("source", "Unknown"),
                "score": getattr(doc, 'score', 1.0)
            } for doc in retrieved_docs]
            
            return {
                "answer": response,
                "sources": sources
            }
            
        except Exception as e:
            print(f"‚ùå Error in query: {e}")
            return {
                "answer": ERROR_RESPONSE,
                "sources": []
            }
    
    async def aquery(self, query: str, system_prompt: Optional[str] = None) -> Dict[str, Any]:
        """
        Async version of query method
        """
        # For now, use sync version (can be enhanced later with async retrievers)
        return self.query(query, system_prompt)
    
    def _format_context(self, docs: List[Document]) -> str:
        """
        Format retrieved documents into context string
        """
        formatted_results = []
        for i, doc in enumerate(docs):
            try:
                text = doc.page_content
                source = doc.metadata.get("source", "Unknown")
                formatted_results.append(f"Document {i+1} (Source: {source}):\n{text}\n")
            except Exception:
                continue
        
        return "\n".join(formatted_results)
    
    def get_retriever_info(self) -> Dict[str, Any]:
        """
        Get information about the current retriever setup
        """
        return {
            "retriever_type": self.retriever_type,
            "chunk_size": self.chunk_size,
            "chunk_overlap": self.chunk_overlap,
            "k": self.k,
            "num_documents": len(self.docs),
            "model_name": self.model_name
        }
